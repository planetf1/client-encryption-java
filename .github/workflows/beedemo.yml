name: Bee Agent Demo
'on':
  pull_request:
    branches:
      - "**"
env:
  OLLAMA_HOST: ollama.cherrybyte.me.uk
  OLLAMA_PORT: 11434
  OLLAMA_MODEL: llama3.2
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check connectivity to OLLAMA
        run: |
          nc -zv ${OLLAMA_HOST} ${OLLAMA_PORT}
      - name: Install dependencies
        run: |
          pip install ollama
      - name: Verify LLM
        shell: python
        run: |
          import os
          from ollama import Client
          url = 'http://' + os.getenv("OLLAMA_HOST") + ':' + os.getenv("OLLAMA_PORT")
          client = Client(host=url)
          response = client.chat(model=os.getenv("OLLAMA_MODEL"), messages=[
            {
              'role': 'user',
              'content': 'Generate a simple python application which prints out a simple message to the screen. Add comments to each line to explain what it does',
            },
          ])
          print(response)
